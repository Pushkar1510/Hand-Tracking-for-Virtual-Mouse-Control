{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4233900d-4399-456f-b5b2-b115ab470868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-07 02:22:22.777 Python[5689:276061] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738912943.857397  276061 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n",
      "I0000 00:00:1738912943.861701  276061 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1738912943.904217  276299 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738912943.905592  276305 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738912943.908461  276299 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738912943.911073  276305 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1738912943.917772  276301 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "2025-02-07 02:22:24.468 Python[5689:276061] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-02-07 02:22:24.468 Python[5689:276061] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils  # Drawing utilities\n",
    "\n",
    "# Capture video from webcam\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Get screen dimensions\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Define Landmark Indices for Face\n",
    "EYE_LANDMARKS = [33, 133, 362, 263]\n",
    "LIP_LANDMARKS = [61, 146, 291, 308]\n",
    "\n",
    "# Smoothing parameters for mouse control\n",
    "smooth_factor = 5\n",
    "prev_x, prev_y = 0, 0\n",
    "\n",
    "# Initialize Face Mesh and Hand Tracking\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh, \\\n",
    "     mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()  # Read frame\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)  # Flip the frame for a mirror effect\n",
    "        h, w, _ = frame.shape  # Get frame dimensions\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "\n",
    "        # Process face and hands\n",
    "        face_results = face_mesh.process(rgb_frame)\n",
    "        hand_results = hands.process(rgb_frame)\n",
    "\n",
    "        # Draw Face Mesh Landmarks\n",
    "        if face_results.multi_face_landmarks:\n",
    "            for face_landmarks in face_results.multi_face_landmarks:\n",
    "                # Draw face mesh\n",
    "                mp_drawing.draw_landmarks(frame, face_landmarks, mp_face_mesh.FACEMESH_TESSELATION)\n",
    "\n",
    "                # Draw Eye Landmarks\n",
    "                for landmark_idx in EYE_LANDMARKS:\n",
    "                    landmark = face_landmarks.landmark[landmark_idx]\n",
    "                    x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                    cv2.circle(frame, (x, y), 3, (0, 255, 255), -1)  # Yellow for eyes\n",
    "\n",
    "                # Draw Lip Landmarks\n",
    "                for landmark_idx in LIP_LANDMARKS:\n",
    "                    landmark = face_landmarks.landmark[landmark_idx]\n",
    "                    x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                    cv2.circle(frame, (x, y), 3, (0, 0, 255), -1)  # Red for lips\n",
    "\n",
    "        # Process Hand Landmarks\n",
    "        if hand_results.multi_hand_landmarks:\n",
    "            for hand_landmarks in hand_results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                # Get index finger tip and thumb tip\n",
    "                index_finger_tip = hand_landmarks.landmark[8]  # Index finger tip\n",
    "                thumb_tip = hand_landmarks.landmark[4]  # Thumb tip\n",
    "\n",
    "                # Convert coordinates to screen space\n",
    "                x_index = int(index_finger_tip.x * w)\n",
    "                y_index = int(index_finger_tip.y * h)\n",
    "                x_thumb = int(thumb_tip.x * w)\n",
    "                y_thumb = int(thumb_tip.y * h)\n",
    "\n",
    "                # Convert to screen resolution\n",
    "                screen_x = np.interp(x_index, [0, w], [0, screen_width])\n",
    "                screen_y = np.interp(y_index, [0, h], [0, screen_height])\n",
    "\n",
    "                # Smooth cursor movement\n",
    "                curr_x = (prev_x + screen_x) / smooth_factor\n",
    "                curr_y = (prev_y + screen_y) / smooth_factor\n",
    "                prev_x, prev_y = curr_x, curr_y\n",
    "\n",
    "                # Move mouse cursor\n",
    "                pyautogui.moveTo(int(curr_x), int(curr_y), duration=0.1)\n",
    "\n",
    "                # Check for clicking gesture (if index and thumb are close)\n",
    "                distance = np.linalg.norm(np.array([x_index, y_index]) - np.array([x_thumb, y_thumb]))\n",
    "\n",
    "                if distance < 30:  # Click when fingers are close\n",
    "                    pyautogui.click()\n",
    "                    cv2.putText(frame, \"Click\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Face & Hand Landmarks with Virtual Mouse', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0dc2d6-0fdd-415a-9ba1-7abe0443bf32",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dda66f-0ae1-4684-aed9-8a13b81418eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
